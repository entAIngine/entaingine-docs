"use strict";(self.webpackChunkentaingine_docs=self.webpackChunkentaingine_docs||[]).push([[4104],{8:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>h});var i=t(4848),s=t(8453);const o={title:"Few-Shot Prompting",slug:"/prompting-techniques/few-shot",sidebar_position:3,description:"Guide the model with a few examples to shape its output."},r=void 0,a={id:"prompting-techniques/few-shot",title:"Few-Shot Prompting",description:"Guide the model with a few examples to shape its output.",source:"@site/docs/prompting-techniques/few-shot.mdx",sourceDirName:"prompting-techniques",slug:"/prompting-techniques/few-shot",permalink:"/prompting-techniques/few-shot",draft:!1,unlisted:!1,editUrl:"https://github.com/entAIngine/entaingine-docs/tree/main/docs/prompting-techniques/few-shot.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Few-Shot Prompting",slug:"/prompting-techniques/few-shot",sidebar_position:3,description:"Guide the model with a few examples to shape its output."},sidebar:"tutorialSidebar",previous:{title:"Zero-Shot Prompting",permalink:"/prompting-techniques/zero-shot"},next:{title:"Chain-of-Thought Prompting",permalink:"/prompting-techniques/chain-of-thought"}},l={},h=[{value:"Use When",id:"use-when",level:2},{value:"Pattern",id:"pattern",level:2},{value:"Examples",id:"examples",level:2},{value:"Example 1: Capital Cities (from Brown et al., 2020)",id:"example-1-capital-cities-from-brown-et-al-2020",level:3},{value:"Example 2: Grammar Correction",id:"example-2-grammar-correction",level:3},{value:"Example 3: Sentiment Analysis (new)",id:"example-3-sentiment-analysis-new",level:3},{value:"Benefits",id:"benefits",level:2},{value:"\u26a0\ufe0f Pitfalls",id:"\ufe0f-pitfalls",level:2},{value:"Few-Shot vs Zero-Shot Comparison",id:"few-shot-vs-zero-shot-comparison",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Few-Shot Prompting is a technique where the model is provided with a small number of high-quality examples (typically 2\u20135) before being asked to perform a similar task. By seeing these demonstrations, the model can better understand the desired output format, style, or logic, leading to improved performance\u2014especially for tasks that are less common or more complex."}),"\n",(0,i.jsx)(n.p,{children:"Few-shot prompting is a core method in prompt engineering, enabling in-context learning and adaptation without retraining the model."}),"\n",(0,i.jsx)(n.h2,{id:"use-when",children:"Use When"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The task benefits from context or demonstration."}),"\n",(0,i.jsx)(n.li,{children:"You want to guide the model\u2019s style, format, or logic."}),"\n",(0,i.jsx)(n.li,{children:"The model struggles with zero-shot performance."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"pattern",children:"Pattern"}),"\n",(0,i.jsx)(n.p,{children:"Show 2\u20135 examples of input-output pairs, then prompt the model to continue in the same way."}),"\n",(0,i.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(n.h3,{id:"example-1-capital-cities-from-brown-et-al-2020",children:"Example 1: Capital Cities (from Brown et al., 2020)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Q: What is the capital of France?\nA: Paris\nQ: What is the capital of Germany?\nA: Berlin\nQ: What is the capital of Italy?\nA:\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Model output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Rome\n"})}),"\n",(0,i.jsx)(n.h3,{id:"example-2-grammar-correction",children:"Example 2: Grammar Correction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:'Correct the grammar:\nInput: "She go to school every day."\nOutput: She goes to school every day.\nInput: "They is playing outside."\nOutput:\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Model output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"They are playing outside.\n"})}),"\n",(0,i.jsx)(n.h3,{id:"example-3-sentiment-analysis-new",children:"Example 3: Sentiment Analysis (new)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:'Classify the sentiment:\nText: "I love this product!"\nSentiment: Positive\nText: "The service was disappointing."\nSentiment: Negative\nText: "It was okay, nothing special."\nSentiment:\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Model output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Neutral\n"})}),"\n",(0,i.jsx)(n.h2,{id:"benefits",children:"Benefits"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Improved accuracy:"})," Demonstrations help the model generalize to new but similar tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control:"})," You can steer the model\u2019s output style, logic, or format."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Flexibility:"})," Works for a wide range of tasks, from classification to generation."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\ufe0f-pitfalls",children:"\u26a0\ufe0f Pitfalls"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Too many examples can exceed context limits"}),"\n",(0,i.jsx)(n.li,{children:"Examples should be relevant, clear, and diverse"}),"\n",(0,i.jsx)(n.li,{children:"The model may overfit to the examples if they are not representative"}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{title:"Important",type:"warning",children:(0,i.jsx)(n.p,{children:"Choose diverse, high-quality examples that represent the full scope of your task. 2-5 examples are usually optimal!"})}),"\n",(0,i.jsx)(n.h2,{id:"few-shot-vs-zero-shot-comparison",children:"Few-Shot vs Zero-Shot Comparison"}),"\n",(0,i.jsx)(n.p,{children:"The following diagram illustrates the key differences between approaches:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'ZERO-SHOT PROMPTING:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Task: "Classify sentiment: \'I love  \u2502\n\u2502 this product!\' "                    \u2502\n\u2502                                     \u2502\n\u2502 Model: (relies on pre-training only)\u2502\n\u2502 Output: "Positive"                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nFEW-SHOT PROMPTING:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Examples:                           \u2502\n\u2502 "Great service!" \u2192 Positive         \u2502\n\u2502 "Terrible quality" \u2192 Negative       \u2502\n\u2502 "It\'s okay" \u2192 Neutral              \u2502\n\u2502                                     \u2502\n\u2502 Task: "Classify sentiment: \'I love  \u2502\n\u2502 this product!\' "                    \u2502\n\u2502                                     \u2502\n\u2502 Model: (learns from examples)       \u2502\n\u2502 Output: "Positive" (more confident) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nBenefits of Few-Shot:\n\u2022 Better accuracy through examples\n\u2022 Consistent formatting\n\u2022 Domain-specific adaptation\n\u2022 Reduced ambiguity\n'})}),"\n",(0,i.jsx)(n.p,{children:"This demonstrates how examples provide crucial context that improves model performance and consistency."}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2005.14165",children:"Language Models are Few-Shot Learners"}),". ",(0,i.jsx)(n.em,{children:"NeurIPS 2020"}),"."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);