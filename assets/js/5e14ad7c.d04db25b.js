"use strict";(self.webpackChunkentaingine_docs=self.webpackChunkentaingine_docs||[]).push([[5459],{8964:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var t=i(4848),a=i(8453);const s={title:"Multimodal Chain-of-Thought",slug:"/prompting-techniques/multimodal-cot",sidebar_position:16,description:"Combine visual and textual reasoning for complex multimodal problems."},o=void 0,r={id:"prompting-techniques/multimodal-cot",title:"Multimodal Chain-of-Thought",description:"Combine visual and textual reasoning for complex multimodal problems.",source:"@site/docs/prompting-techniques/multimodal-cot.mdx",sourceDirName:"prompting-techniques",slug:"/prompting-techniques/multimodal-cot",permalink:"/prompting-techniques/multimodal-cot",draft:!1,unlisted:!1,editUrl:"https://github.com/entAIngine/entaingine-docs/tree/main/docs/prompting-techniques/multimodal-cot.mdx",tags:[],version:"current",sidebarPosition:16,frontMatter:{title:"Multimodal Chain-of-Thought",slug:"/prompting-techniques/multimodal-cot",sidebar_position:16,description:"Combine visual and textual reasoning for complex multimodal problems."},sidebar:"tutorialSidebar",previous:{title:"ReAct (Reasoning and Acting)",permalink:"/prompting-techniques/react"},next:{title:"Graph Prompting",permalink:"/prompting-techniques/graph-prompting"}},l={},c=[{value:"\ud83c\udfaf Core Principles",id:"-core-principles",level:2},{value:"Types of Multimodal Reasoning",id:"types-of-multimodal-reasoning",level:2},{value:"Visual Question Answering with Reasoning",id:"visual-question-answering-with-reasoning",level:3},{value:"Document Understanding and Analysis",id:"document-understanding-and-analysis",level:3},{value:"Scientific and Technical Analysis",id:"scientific-and-technical-analysis",level:3},{value:"Advanced MM-CoT Techniques",id:"advanced-mm-cot-techniques",level:2},{value:"Multi-Step Visual Problem Solving",id:"multi-step-visual-problem-solving",level:3},{value:"Comparative Visual Analysis",id:"comparative-visual-analysis",level:3},{value:"Spatial and Temporal Reasoning",id:"spatial-and-temporal-reasoning",level:3},{value:"Implementation Strategies",id:"implementation-strategies",level:2},{value:"Structured Visual Analysis Framework",id:"structured-visual-analysis-framework",level:3},{value:"Error Prevention in Multimodal Reasoning",id:"error-prevention-in-multimodal-reasoning",level:3},{value:"Domain-Specific Applications",id:"domain-specific-applications",level:2},{value:"Educational Content Analysis",id:"educational-content-analysis",level:3},{value:"Medical Imaging Analysis",id:"medical-imaging-analysis",level:3},{value:"Business Intelligence and Analytics",id:"business-intelligence-and-analytics",level:3},{value:"Best Practices for MM-CoT Implementation",id:"best-practices-for-mm-cot-implementation",level:2},{value:"Visual Attention and Description",id:"visual-attention-and-description",level:3},{value:"Cross-Modal Integration",id:"cross-modal-integration",level:3},{value:"Quality Assurance",id:"quality-assurance",level:3},{value:"Limitations and Considerations",id:"limitations-and-considerations",level:2},{value:"Technical Limitations",id:"technical-limitations",level:3},{value:"Cognitive Challenges",id:"cognitive-challenges",level:3},{value:"Future Developments",id:"future-developments",level:2},{value:"Enhanced Integration Techniques",id:"enhanced-integration-techniques",level:3},{value:"Tool Integration",id:"tool-integration",level:3},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Multimodal Chain-of-Thought Framework",src:i(3103).A+"",width:"912",height:"662"})}),"\n",(0,t.jsx)(n.p,{children:"Multimodal Chain-of-Thought (MM-CoT) prompting is an advanced technique that combines visual and textual information to guide language models through step-by-step reasoning across multiple modalities. This approach enables models to leverage both image understanding and text comprehension to solve complex problems that require integrated analysis of visual and linguistic elements."}),"\n",(0,t.jsxs)(n.p,{children:["Introduced by ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2302.00923",children:"Zhang et al. (2023)"}),", MM-CoT significantly improves performance on multimodal reasoning tasks by explicitly encouraging models to articulate their reasoning process while drawing connections between visual elements and textual context."]}),"\n",(0,t.jsx)(n.h2,{id:"-core-principles",children:"\ud83c\udfaf Core Principles"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal Chain-of-Thought operates on several key principles:"}),"\n",(0,t.jsx)(n.admonition,{title:"The Four Pillars of MM-CoT",type:"note",children:(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Modal Integration"}),": Seamlessly combining information from visual and textual sources"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explicit Reasoning"}),": Articulating step-by-step thought processes that reference both modalities"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Grounding"}),": Connecting abstract reasoning to specific visual elements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Contextual Synthesis"}),": Building comprehensive understanding through modal interaction"]}),"\n"]})}),"\n",(0,t.jsx)(n.h2,{id:"types-of-multimodal-reasoning",children:"Types of Multimodal Reasoning"}),"\n",(0,t.jsx)(n.h3,{id:"visual-question-answering-with-reasoning",children:"Visual Question Answering with Reasoning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: A photograph showing a crowded train station with people wearing winter coats and carrying umbrellas]\n\nQuestion: What can you infer about the weather conditions when this photo was taken?\n\nMultimodal CoT Response:\nStep 1: I observe that people in the image are wearing heavy winter coats and jackets, suggesting cold temperatures.\nStep 2: Many individuals are carrying umbrellas, which indicates precipitation (rain or snow).\nStep 3: The combination of winter clothing and umbrellas suggests it's likely a cold, wet day.\nStep 4: The indoor setting of the train station might also indicate people are seeking shelter from harsh weather outside.\n\nConclusion: Based on the visual evidence of winter clothing and umbrellas, the weather was likely cold and rainy or snowy when this photo was taken.\n"})}),"\n",(0,t.jsx)(n.h3,{id:"document-understanding-and-analysis",children:"Document Understanding and Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: A business report page with charts, graphs, and text sections]\n\nQuestion: What are the key insights from this quarterly report, and what trends can you identify?\n\nMultimodal CoT Response:\nStep 1: I'll examine the title and headers to understand the report structure and timeframe.\nStep 2: Looking at the bar chart on the left, I can see quarterly revenue data showing an upward trend from Q1 to Q3.\nStep 3: The pie chart on the right shows market share distribution, with three main segments.\nStep 4: The text mentions specific percentage increases, which I'll correlate with the visual data.\nStep 5: Combining the textual financial data with the visual trends, I can identify growth patterns and areas of concern.\n\nKey Insights: The report shows 15% revenue growth over three quarters, with Product A maintaining 45% market share. The upward trend in the bar chart aligns with the textual data indicating successful expansion in two key markets.\n"})}),"\n",(0,t.jsx)(n.h3,{id:"scientific-and-technical-analysis",children:"Scientific and Technical Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: A laboratory setup with beakers, measuring equipment, and a data readout display]\n\nQuestion: Analyze this experimental setup and predict what type of experiment is being conducted.\n\nMultimodal CoT Response:\nStep 1: I observe multiple glass beakers with different colored solutions, suggesting chemical mixing or reactions.\nStep 2: There's a digital scale visible, indicating precise measurement is important for this experiment.\nStep 3: The data readout display shows pH readings and temperature values, suggesting monitoring of chemical properties.\nStep 4: The presence of safety equipment (gloves, goggles visible in frame) indicates this involves potentially hazardous chemicals.\nStep 5: The systematic arrangement and measurement tools suggest this is a controlled, quantitative experiment.\n\nAnalysis: This appears to be a chemical titration or pH analysis experiment where precise measurements and monitoring of chemical properties are crucial for determining substance concentrations or reaction endpoints.\n"})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-mm-cot-techniques",children:"Advanced MM-CoT Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"multi-step-visual-problem-solving",children:"Multi-Step Visual Problem Solving"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: A complex architectural blueprint with measurements, annotations, and multiple views]\n\nQuestion: Calculate the total square footage of the main living area and identify any potential design issues.\n\nMultimodal CoT Response:\nStep 1: I'll identify the main living area boundaries by examining the floor plan layout and room labels.\nStep 2: From the blueprint, I can see the living room dimensions marked as 20' \xd7 15' and the adjacent dining area as 12' \xd7 14'.\nStep 3: I'll calculate each area: Living room = 20 \xd7 15 = 300 sq ft; Dining area = 12 \xd7 14 = 168 sq ft.\nStep 4: Checking the blueprint annotations, these areas appear to be connected as one open space.\nStep 5: Total main living area = 300 + 168 = 468 square feet.\nStep 6: Looking for design issues: I notice the kitchen entrance creates a narrow walkway (marked as 3' wide) which might create traffic flow problems.\nStep 7: The window placement on the north wall might limit furniture arrangement options.\n\nResult: Total main living area is 468 sq ft. Potential issues include the narrow kitchen entrance passage and limited furniture placement due to north-facing window positioning.\n"})}),"\n",(0,t.jsx)(n.h3,{id:"comparative-visual-analysis",children:"Comparative Visual Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Images: Two similar charts showing company performance data for 2022 and 2023]\n\nQuestion: Compare these annual performance charts and identify the most significant changes.\n\nMultimodal CoT Response:\nStep 1: I'll examine the chart structures to ensure I'm comparing equivalent metrics between the two years.\nStep 2: Looking at the revenue bars, 2023 shows approximately 25% higher values than 2022 across all quarters.\nStep 3: The profit margin line graph shows 2022 had more volatility, while 2023 demonstrates steadier growth.\nStep 4: In the 2023 chart, I notice a new product category wasn't present in 2022's data.\nStep 5: The color coding shows different market segments, and the blue segment (international sales) appears significantly larger in 2023.\nStep 6: The expense breakdown pie charts show a shift toward higher R&D spending in 2023 (18% vs 12% in 2022).\n\nKey Changes: Revenue increased 25%, profit margins stabilized, international sales expanded significantly, and R&D investment increased from 12% to 18% of total expenses, indicating strategic growth focus.\n"})}),"\n",(0,t.jsx)(n.h3,{id:"spatial-and-temporal-reasoning",children:"Spatial and Temporal Reasoning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: A time-lapse satellite image showing urban development over several years]\n\nQuestion: Analyze the urban growth patterns and predict future development trends.\n\nMultimodal CoT Response:\nStep 1: I'll examine the earliest timeframe image to establish the baseline urban footprint.\nStep 2: Tracking the progression through subsequent time periods, I can see development expanding primarily toward the southeast.\nStep 3: The expansion pattern follows major transportation routes visible in the imagery.\nStep 4: Green spaces (parks) remain largely unchanged, suggesting zoning protection.\nStep 5: The rate of expansion appears to accelerate in the later time periods based on the density of new development.\nStep 6: Industrial areas (rectangular buildings) are concentrating in the northern section.\nStep 7: Residential development (smaller, scattered buildings) follows transportation corridors.\n\nPrediction: Future development will likely continue southeastern expansion along transportation routes, with industrial development concentrating north and residential growth following corridor patterns while respecting protected green spaces.\n"})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-strategies",children:"Implementation Strategies"}),"\n",(0,t.jsx)(n.h3,{id:"structured-visual-analysis-framework",children:"Structured Visual Analysis Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Framework for consistent multimodal reasoning:\n\n1. **Visual Inventory**: Systematically catalog all visual elements\n2. **Text Integration**: Connect textual information with visual observations\n3. **Cross-Modal Connections**: Identify relationships between visual and textual elements\n4. **Progressive Analysis**: Build understanding step by step\n5. **Synthesis**: Combine insights from all modalities for final conclusions\n\nExample Application:\n[Image: Medical X-ray with patient report text]\n\nStep 1 (Visual Inventory): I see a chest X-ray showing lung fields, heart silhouette, and rib cage.\nStep 2 (Text Integration): The report mentions patient symptoms of chest pain and shortness of breath.\nStep 3 (Cross-Modal Connections): The symptoms align with visual indicators I should examine.\nStep 4 (Progressive Analysis): I'll systematically examine lung clarity, heart size, and any abnormalities.\nStep 5 (Synthesis): Combining visual findings with reported symptoms to form assessment.\n"})}),"\n",(0,t.jsx)(n.h3,{id:"error-prevention-in-multimodal-reasoning",children:"Error Prevention in Multimodal Reasoning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'Common pitfalls and prevention strategies:\n\n1. **Visual Misinterpretation**: \n   Prevention: Describe what you see before interpreting meaning\n   \n2. **Modal Isolation**: \n   Prevention: Explicitly connect visual and textual elements\n   \n3. **Assumption Errors**: \n   Prevention: Distinguish between direct observations and inferences\n   \n4. **Context Neglect**: \n   Prevention: Consider the broader context of both visual and textual information\n\nExample with Error Prevention:\n[Image: Graph showing declining trend line]\n\nIncorrect: "This shows the company is failing."\nCorrect: "I observe a declining trend line in this graph. To properly interpret this, I need to consider: what metric is being measured (from the axis labels), the time period (from the x-axis), and any accompanying text that provides context about what this decline represents."\n'})}),"\n",(0,t.jsx)(n.h2,{id:"domain-specific-applications",children:"Domain-Specific Applications"}),"\n",(0,t.jsx)(n.h3,{id:"educational-content-analysis",children:"Educational Content Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: Textbook page with diagrams, formulas, and explanatory text]\n\nQuestion: Explain the concept being taught and how the visual elements support understanding.\n\nEducational MM-CoT:\nStep 1: I identify this as a physics page about wave motion based on the chapter heading.\nStep 2: The diagram shows sinusoidal waves with labeled wavelength and amplitude markers.\nStep 3: The mathematical formulas relate wave speed to frequency and wavelength (v = f\u03bb).\nStep 4: The text explains how wave properties affect real-world phenomena like sound and light.\nStep 5: The visual elements reinforce the mathematical relationships by showing measurable wave characteristics.\n\nExplanation: This page teaches wave physics by combining visual wave diagrams with mathematical formulas and practical examples, helping students understand abstract concepts through multiple learning modalities.\n"})}),"\n",(0,t.jsx)(n.h3,{id:"medical-imaging-analysis",children:"Medical Imaging Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: Medical scan with accompanying patient history and symptoms]\n\nMedical MM-CoT Reasoning:\nStep 1: Review patient history and presenting symptoms from the text.\nStep 2: Systematically examine the imaging for relevant anatomical structures.\nStep 3: Identify any abnormalities or areas of concern in the visual data.\nStep 4: Correlate imaging findings with clinical symptoms.\nStep 5: Consider differential diagnoses based on combined evidence.\nStep 6: Recommend additional testing or follow-up based on integrated analysis.\n"})}),"\n",(0,t.jsx)(n.h3,{id:"business-intelligence-and-analytics",children:"Business Intelligence and Analytics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"[Image: Dashboard with multiple charts, KPIs, and trend indicators]\n\nBusiness MM-CoT Analysis:\nStep 1: Identify key performance indicators displayed in the dashboard.\nStep 2: Analyze trend directions in time-series charts.\nStep 3: Examine comparative metrics across different business units or time periods.\nStep 4: Connect quantitative visual data with qualitative business context.\nStep 5: Identify correlations between different metrics shown.\nStep 6: Synthesize insights for strategic decision-making recommendations.\n"})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices-for-mm-cot-implementation",children:"Best Practices for MM-CoT Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"visual-attention-and-description",children:"Visual Attention and Description"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Systematic Scanning"}),": Develop consistent patterns for examining visual information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Descriptive Clarity"}),": Describe visual elements before interpreting them"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Detail Hierarchy"}),": Start with major elements, then focus on relevant details"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Spatial Awareness"}),": Consider spatial relationships and layout significance"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"cross-modal-integration",children:"Cross-Modal Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explicit Connections"}),": Clearly state how visual and textual information relate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evidence Triangulation"}),": Use multiple sources to support conclusions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Contradiction Resolution"}),": Address conflicts between visual and textual information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Preservation"}),": Maintain awareness of the broader context throughout analysis"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"quality-assurance",children:"Quality Assurance"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Self-Checking Framework:\n1. Did I accurately describe what I see in the image?\n2. Have I properly integrated textual information?\n3. Are my connections between modalities logical and well-supported?\n4. Have I distinguished between observations and inferences?\n5. Does my final conclusion synthesize all available evidence?\n"})}),"\n",(0,t.jsx)(n.h2,{id:"limitations-and-considerations",children:"Limitations and Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"technical-limitations",children:"Technical Limitations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Image Resolution"}),": Poor quality images can limit analysis accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Capabilities"}),": Not all language models can process visual information effectively"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Windows"}),": Large images and extensive text may exceed processing limits"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modality Alignment"}),": Ensuring visual and textual information are properly synchronized"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"cognitive-challenges",children:"Cognitive Challenges"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Attention Management"}),": Balancing focus between visual and textual elements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Bias Prevention"}),": Avoiding overreliance on one modality"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complexity Scaling"}),": Managing reasoning chains as problem complexity increases"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Propagation"}),": Mistakes in early steps can compound through the reasoning chain"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"future-developments",children:"Future Developments"}),"\n",(0,t.jsx)(n.h3,{id:"enhanced-integration-techniques",children:"Enhanced Integration Techniques"}),"\n",(0,t.jsx)(n.p,{children:"Research continues into:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic Attention"}),": Adaptively focusing on relevant visual regions based on reasoning needs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Scale Analysis"}),": Reasoning across different levels of visual detail"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temporal Sequences"}),": Extending MM-CoT to video and time-series visual data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interactive Reasoning"}),": Enabling back-and-forth refinement between visual and textual analysis"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"tool-integration",children:"Tool Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automated Visual Analysis"}),": Integrating computer vision tools to augment human-interpretable reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Modal Search"}),": Using reasoning outcomes to guide additional information retrieval"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collaborative Reasoning"}),": Combining multiple models or agents for complex multimodal problems"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Zhang, Z., et al. (2023). Multimodal Chain-of-Thought Reasoning in Language Models. arXiv preprint arXiv:2302.00923"}),"\n",(0,t.jsx)(n.li,{children:"Li, J., et al. (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. ICML 2022"}),"\n",(0,t.jsx)(n.li,{children:"Lu, J., et al. (2022). Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks. arXiv preprint arXiv:2206.08916"}),"\n",(0,t.jsx)(n.li,{children:"Alayrac, J., et al. (2022). Flamingo: a Visual Language Model for Few-Shot Learning. NeurIPS 2022"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},3103:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/multimodal-cot-34dccbf580471ca3d6eba86ebcdac01c.png"},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var t=i(6540);const a={},s=t.createContext(a);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);