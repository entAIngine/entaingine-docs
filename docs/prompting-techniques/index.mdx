---
title: Prompting Techniques
slug: /prompting-techniques
sidebar_position: 1
description: Overview of prompting techniques for effective use of language models.
---

Explore practical prompting techniques to get the most out of language models. Each technique is summarized below with a link to detailed guidance and examples.

- [Zero-Shot Prompting](./zero-shot): Ask the model to perform a task without examples.
- [Few-Shot Prompting](./few-shot): Provide a few examples to guide the model’s behavior.
- [Chain-of-Thought](./chain-of-thought): Encourage step-by-step reasoning for complex tasks.
- [Self-Consistency](./self-consistency): Generate multiple solutions and select the most consistent answer.
- [Generated Knowledge](./generated-knowledge): Use model-generated facts or context to improve responses.
- [Prompt Chaining](./prompt-chaining): Link multiple prompts to solve multi-step problems.
- [Tree of Thoughts](./tree-of-thoughts): Explore multiple reasoning paths for better solutions.
- [Retrieval-Augmented Generation (RAG)](./rag): Incorporate external knowledge into responses.
- [ART (Automatic Reasoning and Tool-use)](./art): Combine reasoning with tool use for complex tasks.
- [Automatic Prompt Engineer](./automatic-prompt-engineer): Use models to generate and refine prompts automatically.
- [Active Prompt](./active-prompt): Dynamically adjust prompts based on feedback or context.
- [Directional Stimulus Prompting](./directional-stimulus): Guide the model’s output with explicit cues.
- [PAL (Program-Aided Language)](./pal): Use code or programs to help the model solve problems.
- [ReAct](./react): Integrate reasoning and action for interactive tasks.
- [Multimodal Chain-of-Thought](./multimodal-cot): Combine text and images for stepwise reasoning.
- [Graph Prompting](./graph-prompting): Structure prompts as graphs to represent relationships.